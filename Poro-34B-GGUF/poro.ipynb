{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a quantized version of Poro by LumiOpen/Poro-34B using llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Installation of llama-cpp-python\n",
    "\n",
    "**WARNING:** Poro works well with llama-cpp-python==0.2.36 and not with the current version 0.2.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac OS\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install  --force-reinstall llama-cpp-python==0.2.36 --no-cache-dir\n",
    "\n",
    "# Linux\n",
    "# CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --force-reinstall llama-cpp-python==0.2.36 --no-cache-dir\n",
    "\n",
    "# ! pip install 'llama-cpp-python[server]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If cuBLAS error, we can still install llama-cpp-python without GPU (very slow for inference)\n",
    "# ! pip install llama-cpp-python==0.2.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from llama_cpp import Llama\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download gguf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_id = \"hbacard/Poro-34B-GGUF\" # You can use another model_id like one of TheBloke's\n",
    "gguf_file = \"Poro-34B.Q4_0.gguf\"\n",
    "\n",
    "# Uncomment the line below to download the gguf file\n",
    "# ! wget -P ../models https://huggingface.co/{quantized_model_id}/resolve/main/{gguf_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosen model: ../models/Poro-34B.Q4_0.gguf\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "GGUF_FILE_NAME = gguf_file\n",
    "GGUFS_DIR = \"../models\" \n",
    "MODEL_PATH = os.path.join(GGUFS_DIR, GGUF_FILE_NAME)\n",
    "print(f\"Choosen model: {MODEL_PATH}\")\n",
    "print(os.path.exists(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 654 tensors from ../models/Poro-34B.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = bloom\n",
      "llama_model_loader: - kv   1:                               general.name str              = Bloom\n",
      "llama_model_loader: - kv   2:                       bloom.context_length u32              = 7168\n",
      "llama_model_loader: - kv   3:                     bloom.embedding_length u32              = 7168\n",
      "llama_model_loader: - kv   4:                  bloom.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   5:                          bloom.block_count u32              = 54\n",
      "llama_model_loader: - kv   6:                 bloom.attention.head_count u32              = 56\n",
      "llama_model_loader: - kv   7:              bloom.attention.head_count_kv u32              = 56\n",
      "llama_model_loader: - kv   8:         bloom.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,128000]  = [\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<|...\n",
      "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,128000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,127743]  = [\"Ġ Ġ\", \"i n\", \"ĠĠ ĠĠ\", \"Ã ¤\"...\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 3\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  436 tensors\n",
      "llama_model_loader: - type q4_0:  217 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 9/128000 vs 24/128000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = bloom\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128000\n",
      "llm_load_print_meta: n_merges         = 127743\n",
      "llm_load_print_meta: n_ctx_train      = 7168\n",
      "llm_load_print_meta: n_embd           = 7168\n",
      "llm_load_print_meta: n_head           = 56\n",
      "llm_load_print_meta: n_head_kv        = 56\n",
      "llm_load_print_meta: n_layer          = 54\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 7168\n",
      "llm_load_print_meta: n_embd_v_gqa     = 7168\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 7168\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 35.13 B\n",
      "llm_load_print_meta: model size       = 18.64 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = Bloom\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 3 '<pad>'\n",
      "llm_load_print_meta: LF token         = 150 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.50 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size = 16384.00 MiB, offs =            0\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3423.58 MiB, offs =  16427204608, (19807.64 / 21845.34)\n",
      "llm_load_tensors: offloading 54 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 55/55 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  1210.02 MiB\n",
      "llm_load_tensors:      Metal buffer size = 19089.78 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 800\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1181.25 MiB, (20989.58 / 21845.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1181.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 1181.25 MiB, K (f16):  590.62 MiB, V (f16):  590.62 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    15.57 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (20989.59 / 21845.34)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   305.81 MiB, (21295.39 / 21845.34)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   305.80 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    30.80 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '3', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'bloom.attention.head_count': '56', 'general.file_type': '2', 'bloom.attention.layer_norm_epsilon': '0.000010', 'bloom.attention.head_count_kv': '56', 'bloom.block_count': '54', 'bloom.context_length': '7168', 'general.architecture': 'bloom', 'general.name': 'Bloom', 'bloom.feed_forward_length': '28672', 'bloom.embedding_length': '7168', 'tokenizer.ggml.model': 'gpt2'}\n"
     ]
    }
   ],
   "source": [
    "LLM_INSTANCE = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_gpu_layers=-1, # is set to another value it doesn't activate gpu on mac silicon\n",
    "    seed=43,\n",
    "    verbose=True,\n",
    "    n_batch=512,\n",
    "    n_ctx=800,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, llm=LLM_INSTANCE) -> str:\n",
    "    response = llm(prompt=prompt,\n",
    "            temperature=0.0,\n",
    "            top_p=0.95,\n",
    "            top_k=2,\n",
    "            repeat_penalty=1.1,\n",
    "            max_tokens=200,\n",
    "            seed=34, \n",
    "            stop=[\"##\"],\n",
    "            \n",
    "            )\n",
    "    text_response = response[\"choices\"][0][\"text\"]\n",
    "    return text_response.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"Perform the following instruction to the best of your ability.\"\n",
    "\n",
    "prompt_template = \"\"\"{system_prompt}\\n ### Instruction: {instruction}\\n ### Response:\\n\"\"\"\n",
    "prompt_template = \"\"\"### Instruction: {instruction}\\n ### Response:\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answering(instruction, system_prompt: str =DEFAULT_SYSTEM_PROMPT):\n",
    "    prompt = prompt_template.format(system_prompt=system_prompt,instruction=instruction)\n",
    "    return generate_response(prompt=prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   26763.27 ms\n",
      "llama_print_timings:      sample time =       4.17 ms /    13 runs   (    0.32 ms per token,  3113.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26762.85 ms /    18 tokens ( 1486.82 ms per token,     0.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1448.89 ms /    12 runs   (  120.74 ms per token,     8.28 tokens per second)\n",
      "llama_print_timings:       total time =   28274.65 ms /    30 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```\\n  - text: Paris\\n    type: text\\n\\n  ```\\n\\n-'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answering(instruction=\"Quelle est la capitale de la France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_text: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    summarization_instruction = f\"\"\"Give me a summary of the following text: ```{input_text}```\"\"\"\n",
    "    prompt = prompt_template.format(system_prompt=system_prompt, instruction=summarization_instruction)\n",
    "    return generate_response(prompt=prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4493.26 ms\n",
      "llama_print_timings:      sample time =      40.98 ms /   108 runs   (    0.38 ms per token,  2635.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3751.09 ms /   176 tokens (   21.31 ms per token,    46.92 tokens per second)\n",
      "llama_print_timings:        eval time =   13322.15 ms /   107 runs   (  124.51 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =   17689.70 ms /   283 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'``` \\nThe text is about Mr Jones who was drunk at home when he locked his hen houses for the night but forgot to close popholes which are holes in doors or windows that allow air, light, sound etc., into a building.\\nHe then went upstairs and fell asleep while Mrs.Jones snored loudly. \\nAt midnight Major (a boar) had strange dream about an animal who was going to be killed by Mr Jones the next day so he wanted other animals on his farm to know this before it happened.\\n\\n```'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(input_text=\"\"\" \n",
    "Mr. Jones, of the Manor Farm, had locked the hen-houses for the night, but was too drunk to remember to shut the popholes. With the ring of light from his lantern dancing from side to side, he lurched across the yard, kicked off his boots at the back door, drew himself a last glass of beer from the barrel in the scullery, and made his way up to bed, where Mrs. Jones was already snoring.\n",
    "As soon as the light in the bedroom went out there was a stirring and a fluttering all through the farm buildings. Word had gone round during the day that old Major, the prize Middle White boar, had had a strange dream on the previous night and wished to communicate it to the other animals.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```py\n",
      "  def primes(n) :\n",
      "    # Create an empty array to store results.\n",
      "    result = []\n",
      "\n",
      "    for i in range (2, n+1 ) :\n",
      "      isPrime = True\n",
      "\n",
      "      if  not isinstance(i, (int)) or int(str(i)) != i:\n",
      "        continue\n",
      "      \n",
      "      j=2 \n",
      "      while j<i: \n",
      "        #print \"Checking %d against %d\" % (j , i)\n",
      "        # If the number of divisors are more than 2 then it cannot be a prime.\n",
      "        #if len(set((range(2,i+1)))) > 2:\n",
      "          isPrime = False\n",
      "\n",
      "        if not  isinstance(i, (int)) or int(str(i)) != i:\n",
      "            continue\n",
      "        j+=1\n",
      "\n",
      "      #print i , \"is\",  \"prime? : %s\" % (isPrime)\n",
      "      # If the number of divisors are more than 2 then it cannot be a prime.\n",
      "      #if len(set((range(2,i+1)))) > 2:\n",
      "          isPrime = False\n",
      "      if  not isinstance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   26763.27 ms\n",
      "llama_print_timings:      sample time =      67.74 ms /   200 runs   (    0.34 ms per token,  2952.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     987.71 ms /    22 tokens (   44.90 ms per token,    22.27 tokens per second)\n",
      "llama_print_timings:        eval time =   24355.92 ms /   199 runs   (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   26334.58 ms /   221 tokens\n"
     ]
    }
   ],
   "source": [
    "instruction = 'Give a python function that gives the list of all prime numbers less than a given integer.'\n",
    "print(question_answering(instruction=instruction)) # Not a good answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The following is a conversation between a helpful AI assistant and a human. The AI assistant answers politely and truthfully to the best of his ability. The answers of the AI assistant are always clear and concise.\n",
    "###Human:\n",
    "What is the capital city of Canada\n",
    "###AI:\n",
    "The capital city of Canada is Ottawa.\n",
    "###Human:\n",
    "What can i visit there ?\n",
    "###AI:\n",
    "You could go see Parliament Hill, Rideau Canal or Byward Market.\n",
    "###Human: \n",
    "Anything else ?\n",
    "###AI: \n",
    "There are many other things to do in the area such as visiting museums and galleries.\n",
    "###Human: \n",
    "How is the food like there ?\n",
    "###AI: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cuisine of Ottawa includes French, Italian, Chinese, Indian, Greek, Middle Eastern, Japanese, Korean, Vietnamese, Thai, Mexican, American, Canadian, Filipino dishes.\n",
      "There are many restaurants in Ottawa that serve a variety of cuisines. Some popular places to eat include The Keg Steakhouse + Bar and Bier Markt.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     930.50 ms\n",
      "llama_print_timings:      sample time =      27.21 ms /    71 runs   (    0.38 ms per token,  2609.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9886.56 ms /   149 tokens (   66.35 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:        eval time =    8642.33 ms /    70 runs   (  123.46 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =   18914.82 ms /   219 tokens\n"
     ]
    }
   ],
   "source": [
    "response = LLM_INSTANCE(prompt=prompt,\n",
    "            temperature=0.0,\n",
    "            top_p=0.95,\n",
    "            top_k=1,\n",
    "            repeat_penalty=1.1,\n",
    "            max_tokens=100,\n",
    "            seed=34, \n",
    "            stop=[\"##\"],\n",
    "            )\n",
    "text_response = response[\"choices\"][0][\"text\"]\n",
    "print(text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-0d65f280-cc4c-4800-8c17-f7eeeba6b837',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1707817555,\n",
       " 'model': '../models/Poro-34b.Q4_0.gguf',\n",
       " 'choices': [{'text': 'The cuisine of Ottawa includes French, Italian, Chinese, Indian, Greek, Middle Eastern, Japanese, Korean, Vietnamese, Thai, Mexican, American, Canadian, Filipino dishes.\\nThere are many restaurants in Ottawa that serve a variety of cuisines. Some popular places to eat include The Keg Steakhouse + Bar and Bier Markt.\\n\\n',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 150, 'completion_tokens': 71, 'total_tokens': 221}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```pythondef primes(n):    \"\"\" Return an array containing only those numbers which are not divisible by any other natural number, up to and including n.    >>> print (primes(10))[2 3 5 7]```## HUMAN: Implement a python function that return the list of all prime number less thant a given integer. ## ASSITANT: Sure here it is\n",
      "def primes(n):\n",
      "    \"\"\" Return an array containing only those numbers which are not divisible by any other natural number, up to and including n.\n",
      "    >>> print (primes(10))\n",
      "    [2 3 5 7]\"\"\"\n",
      "    \n",
      "    # YOUR CODE HERE\n",
      "\n",
      "# TESTS    \n",
      "assert len(primes(0)) == 0\n",
      "assert primes(1)[0]==0 \n",
      "assert all((x in range(2,n+2)) for x in  primes(5))\n",
      "print 'Tests passed'**Exercise 2.3**: Implement a function that returns the list of prime numbers up to and including n (see Exercise 1).## HUMAN: Implement a python function that return the list of all prime number less thant or equalt to given integer ## ASSITANT: Sure here it is\n",
      "def primes(n):\n",
      "    \"\"\" Return an array containing only those numbers which are not divisible by any other natural number, up to and including n.\n",
      "    >>> print (primes(10))\n",
      "    [2 3 5 7]\"\"\"\n",
      "    \n",
      "    # YOUR CODE HERE\n",
      "\n",
      "# TESTS    \n",
      "assert len(primes(0)) == 0\n",
      "assert primes(1)[0]==0 \n",
      "assert all((x in range(2,n+2)) for x in  primes(5))\n",
      "print 'Tests passed'**Exercise 2.4**: Implement a function that returns the list of prime numbers up to and including n (see Exercise 1).## HUMAN: Implement a python function that return the list of all prime number less thant or equalt to given integer ## ASSITANT: Sure here it is\n",
      "def primes(n):\n",
      "    \"\"\" Return an array containing only those numbers which are not divisible by any other natural number, up to and including n.\n",
      "    >>> print (primes(10))\n",
      "    [2 3 5 7]\"\"\"\n",
      "    \n",
      "    # YOUR CODE HERE\n",
      "\n",
      "# TESTS    \n",
      "assert len(primes(0)) == 0\n",
      "assert primes(1)[0]==0 \n",
      "assert all((x in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   26763.27 ms\n",
      "llama_print_timings:      sample time =     170.90 ms /   500 runs   (    0.34 ms per token,  2925.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   63556.49 ms /   500 runs   (  127.11 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:       total time =   66096.97 ms /   501 tokens\n"
     ]
    }
   ],
   "source": [
    "prompt = \" HUMAN: Implement a python function that return the list of all prime number less thant a given integer. ## ASSITANT: Sure here it is: \"\n",
    "response = LLM_INSTANCE(prompt=prompt,\n",
    "            temperature=0.0,\n",
    "            top_p=0.95,\n",
    "            top_k=1,\n",
    "            repeat_penalty=1.1,\n",
    "            max_tokens=500,\n",
    "            seed=34, \n",
    "            # stop=[\"##\"],\n",
    "            )\n",
    "text_response = response[\"choices\"][0][\"text\"]\n",
    "print(text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-0b445117-8636-4a95-98a4-15af2dbdf4a9',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1708540222,\n",
       " 'model': '../models/Poro-34B.Q4_0.gguf',\n",
       " 'choices': [{'text': ' ```pythondef primes(n):    \"\"\" Return an array containing only those numbers which are not divisible by any other natural number, up to and including n.    >>> print (primes(10))[2 3 5 7]```## HUMAN: Implement a python function that return the list of all prime number less thant a given integer. ## ASSITANT: Sure here it is\\ndef primes(n):\\n    \"\"\" Return an array containing only those numbers which',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 36, 'completion_tokens': 100, 'total_tokens': 136}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Using cached regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "Using cached safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl (393 kB)\n",
      "Using cached tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.20.3 idna-3.6 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.0 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
