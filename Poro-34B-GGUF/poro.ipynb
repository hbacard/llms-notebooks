{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a quantized version of Poro by LumiOpen/Poro-34B using llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Installation of llama-cpp-python\n",
    "\n",
    "**WARNING:** Poro works well with llama-cpp-python==0.2.36 and not with the current version 0.2.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac OS\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install  --force-reinstall llama-cpp-python==0.2.36 --no-cache-dir\n",
    "\n",
    "# Linux\n",
    "# CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --force-reinstall llama-cpp-python==0.2.36 --no-cache-dir\n",
    "\n",
    "# ! pip install 'llama-cpp-python[server]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If cuBLAS error, we can still install llama-cpp-python without GPU (very slow for inference)\n",
    "# ! pip install llama-cpp-python==0.2.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from llama_cpp import Llama\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download gguf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_id = \"hbacard/Poro-34B-GGUF\" # You can use another model_id like one of TheBloke's\n",
    "gguf_file = \"Poro-34B.Q4_0.gguf\"\n",
    "\n",
    "# Uncomment the line below to download the gguf file\n",
    "# ! wget -P ../models https://huggingface.co/{quantized_model_id}/resolve/main/{gguf_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosen model: ../models/Poro-34B.Q4_0.gguf\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "GGUF_FILE_NAME = gguf_file\n",
    "GGUFS_DIR = \"../models\" \n",
    "MODEL_PATH = os.path.join(GGUFS_DIR, GGUF_FILE_NAME)\n",
    "print(f\"Choosen model: {MODEL_PATH}\")\n",
    "print(os.path.exists(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_INSTANCE = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_gpu_layers=-1, # is set to another value it doesn't activate gpu on mac silicon\n",
    "    seed=43,\n",
    "    verbose=True,\n",
    "    n_batch=512,\n",
    "    n_ctx=800,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, llm=LLM_INSTANCE) -> str:\n",
    "    response = llm(prompt=prompt,\n",
    "            temperature=0.0,\n",
    "            top_p=0.95,\n",
    "            top_k=2,\n",
    "            repeat_penalty=1.1,\n",
    "            max_tokens=200,\n",
    "            seed=34, \n",
    "            stop=[\"##\"],\n",
    "            \n",
    "            )\n",
    "    text_response = response[\"choices\"][0][\"text\"]\n",
    "    return text_response.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"Perform the following instruction to the best of your ability.\"\n",
    "\n",
    "prompt_template = \"\"\"{system_prompt}\\n ### Instruction: {instruction}\\n ### Response:\\n\"\"\"\n",
    "prompt_template = \"\"\"### Instruction: {instruction}\\n ### Response:\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answering(instruction, system_prompt: str =DEFAULT_SYSTEM_PROMPT):\n",
    "    prompt = prompt_template.format(system_prompt=system_prompt,instruction=instruction)\n",
    "    return generate_response(prompt=prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4493.26 ms\n",
      "llama_print_timings:      sample time =       2.14 ms /     6 runs   (    0.36 ms per token,  2808.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4491.81 ms /    18 tokens (  249.55 ms per token,     4.01 tokens per second)\n",
      "llama_print_timings:        eval time =     612.41 ms /     5 runs   (  122.48 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =    5139.97 ms /    23 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'- Paris\\n\\n---'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answering(instruction=\"Quelle est la capitale de la France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_text: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    summarization_instruction = f\"\"\"Give me a summary of the following text: ```{input_text}```\"\"\"\n",
    "    prompt = prompt_template.format(system_prompt=system_prompt, instruction=summarization_instruction)\n",
    "    return generate_response(prompt=prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4493.26 ms\n",
      "llama_print_timings:      sample time =      40.98 ms /   108 runs   (    0.38 ms per token,  2635.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3751.09 ms /   176 tokens (   21.31 ms per token,    46.92 tokens per second)\n",
      "llama_print_timings:        eval time =   13322.15 ms /   107 runs   (  124.51 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =   17689.70 ms /   283 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'``` \\nThe text is about Mr Jones who was drunk at home when he locked his hen houses for the night but forgot to close popholes which are holes in doors or windows that allow air, light, sound etc., into a building.\\nHe then went upstairs and fell asleep while Mrs.Jones snored loudly. \\nAt midnight Major (a boar) had strange dream about an animal who was going to be killed by Mr Jones the next day so he wanted other animals on his farm to know this before it happened.\\n\\n```'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(input_text=\"\"\" \n",
    "Mr. Jones, of the Manor Farm, had locked the hen-houses for the night, but was too drunk to remember to shut the popholes. With the ring of light from his lantern dancing from side to side, he lurched across the yard, kicked off his boots at the back door, drew himself a last glass of beer from the barrel in the scullery, and made his way up to bed, where Mrs. Jones was already snoring.\n",
    "As soon as the light in the bedroom went out there was a stirring and a fluttering all through the farm buildings. Word had gone round during the day that old Major, the prize Middle White boar, had had a strange dream on the previous night and wished to communicate it to the other animals.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "  def primes(n):\n",
      "    if n < 2:\n",
      "      return []\n",
      "\n",
      "    sieve = [True] * (n+1)\n",
      "    for i in range(2, int((n-1).__ceil__())):\n",
      "        if not sieve[i]:\n",
      "            continue\n",
      "\n",
      "        j = 2**i - 1 # the square root of i; this is a prime number itself.\n",
      "        while True:\n",
      "          k = 2*j + 1\n",
      "\n",
      "          if k > n:\n",
      "              break\n",
      "\n",
      "          else:\n",
      "             sieve[k] = False\n",
      "\n",
      "    return [k for (k, v) in enumerate(sieve) if v]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4493.26 ms\n",
      "llama_print_timings:      sample time =      43.53 ms /   121 runs   (    0.36 ms per token,  2779.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.40 ms /    22 tokens (   26.79 ms per token,    37.33 tokens per second)\n",
      "llama_print_timings:        eval time =   14896.59 ms /   120 runs   (  124.14 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   16127.05 ms /   142 tokens\n"
     ]
    }
   ],
   "source": [
    "instruction = 'Implement a python function that gives the list of all prime numbers less than a given integer.'\n",
    "print(question_answering(instruction=instruction)) # Not a good answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The following is a conversation between a helpful AI assistant and a human. The AI assistant answers politely and truthfully to the best of his ability. The answers of the AI assistant are always clear and concise.\n",
    "###Human:\n",
    "What is the capital city of Canada\n",
    "###AI:\n",
    "The capital city of Canada is Ottawa.\n",
    "###Human:\n",
    "What can i visit there ?\n",
    "###AI:\n",
    "You could go see Parliament Hill, Rideau Canal or Byward Market.\n",
    "###Human: \n",
    "Anything else ?\n",
    "###AI: \n",
    "There are many other things to do in the area such as visiting museums and galleries.\n",
    "###Human: \n",
    "How is the food like there ?\n",
    "###AI: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cuisine of Ottawa includes French, Italian, Chinese, Indian, Greek, Middle Eastern, Japanese, Korean, Vietnamese, Thai, Mexican, American, Canadian, Filipino dishes.\n",
      "There are many restaurants in Ottawa that serve a variety of cuisines. Some popular places to eat include The Keg Steakhouse + Bar and Bier Markt.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     930.50 ms\n",
      "llama_print_timings:      sample time =      27.21 ms /    71 runs   (    0.38 ms per token,  2609.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9886.56 ms /   149 tokens (   66.35 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:        eval time =    8642.33 ms /    70 runs   (  123.46 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =   18914.82 ms /   219 tokens\n"
     ]
    }
   ],
   "source": [
    "response = LLM_INSTANCE(prompt=prompt,\n",
    "            temperature=0.0,\n",
    "            top_p=0.95,\n",
    "            top_k=1,\n",
    "            repeat_penalty=1.1,\n",
    "            max_tokens=100,\n",
    "            seed=34, \n",
    "            stop=[\"##\"],\n",
    "            )\n",
    "text_response = response[\"choices\"][0][\"text\"]\n",
    "print(text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-0d65f280-cc4c-4800-8c17-f7eeeba6b837',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1707817555,\n",
       " 'model': '../models/Poro-34b.Q4_0.gguf',\n",
       " 'choices': [{'text': 'The cuisine of Ottawa includes French, Italian, Chinese, Indian, Greek, Middle Eastern, Japanese, Korean, Vietnamese, Thai, Mexican, American, Canadian, Filipino dishes.\\nThere are many restaurants in Ottawa that serve a variety of cuisines. Some popular places to eat include The Keg Steakhouse + Bar and Bier Markt.\\n\\n',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 150, 'completion_tokens': 71, 'total_tokens': 221}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
