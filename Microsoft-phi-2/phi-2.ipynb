{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BP3-Wi4kurZ3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.38.0\n",
            "Uninstalling transformers-4.38.0:\n",
            "  Successfully uninstalled transformers-4.38.0\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /private/var/folders/2m/ys1b248x6l93pxxf85kdct740000gn/T/pip-req-build-kwbjuyz4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /private/var/folders/2m/ys1b248x6l93pxxf85kdct740000gn/T/pip-req-build-kwbjuyz4\n",
            "  Resolved https://github.com/huggingface/transformers to commit b338a6c3b8eda29610d4d472cad8cd87cbfdaaed\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: filelock in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from transformers==4.39.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from requests->transformers==4.39.0.dev0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8660843 sha256=c68d8b6cdc5d537907c9990418718cf9f8f65b7d199eb76bb7c926b4ac501d60\n",
            "  Stored in directory: /private/var/folders/2m/ys1b248x6l93pxxf85kdct740000gn/T/pip-ephem-wheel-cache-bpcl8r81/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.39.0.dev0\n",
            "Requirement already satisfied: ipywidgets in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (8.1.2)\n",
            "Requirement already satisfied: comm>=0.1.3 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipywidgets) (8.21.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.10 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipywidgets) (4.0.10)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipywidgets) (3.0.10)\n",
            "Requirement already satisfied: decorator in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers\n",
        "! pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Using cached torch-2.2.1-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from torch) (4.9.0)\n",
            "Collecting sympy (from torch)\n",
            "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: jinja2 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
            "Collecting mpmath>=0.19 (from sympy->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Using cached torch-2.2.1-cp311-none-macosx_11_0_arm64.whl (59.7 MB)\n",
            "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Installing collected packages: mpmath, sympy, networkx, torch\n",
            "Successfully installed mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.2.1\n"
          ]
        }
      ],
      "source": [
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuuiSCotu1Nv",
        "outputId": "8a231210-4651-4e71-d19f-d3c40f49667d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selecting gpu based on system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "def determine_device():\n",
        "    \"\"\"Determines the appropriate device to use for PyTorch based on the environment.\n",
        "\n",
        "    Returns:\n",
        "        str: The device name ('cuda', 'cpu', or 'mps').\n",
        "    \"\"\"\n",
        "    # Check if running in Google Colab or Linux\n",
        "    is_colab_or_linux = os.name == 'posix' and not os.path.exists('/System/Library')  # Checks for MacOS, not Linux\n",
        "\n",
        "    # If running in Google Colab or Linux, check for CUDA availability\n",
        "    if is_colab_or_linux:\n",
        "        try:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        except ImportError:  # In case torch is not installed\n",
        "            device = \"cpu\"\n",
        "    else:  # Running on MacOS\n",
        "        try:\n",
        "            device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "        except ImportError:  # In case torch is not installed or MPS is not available\n",
        "            device = \"cpu\"\n",
        "    return device\n",
        "\n",
        "device = determine_device()\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setting device\n",
        "torch.set_default_device(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "b0c956be1c2c4cc3b5031e28fca4e00c",
            "f45fe69a282d4f0fa1433012c98ff340",
            "5f2cb2c4f449463eb91b0dcd1c73d6e2",
            "09ecf3574401484d88a6ed721c5c81f5",
            "f6506b50b11040ba83b3e6a1b8b3bbe6",
            "e9233c69c0d5441783b85dbc3f633b7c",
            "609b6ef6e5824d398c47be87281f9ff7",
            "b034ccc3e097419fae21d8358ccf605a",
            "53646783d6284efbad5b9f919c63cebf",
            "b28f5f10a24f43dca695797f39ee1672",
            "e2d9dc99c50749a0a6b99c8d4cce3134"
          ]
        },
        "id": "EJcva2CDwK-h",
        "outputId": "b3776ba3-b843-49d7-f9ce-3c5a8fe655c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4667f393c5d649c085b58b3a5a5c5277",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_id = \"microsoft/phi-2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SQcGodBKysQ7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def generate_response(prompt: str, return_full_text: bool = True, split_key: str = \"A:\"):\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors='pt', return_attention_mask=False)\n",
        "        outputs = model.generate(**inputs,\n",
        "                                max_length=1000,\n",
        "                                temperature=0.0,\n",
        "                                top_p=0.95,\n",
        "                                repetition_penalty=1.1,\n",
        "                                top_k=10,\n",
        "                                do_sample=False,\n",
        "                                eos_token_id=tokenizer.eos_token_id,\n",
        "                                pad_token_id=tokenizer.eos_token_id,\n",
        "                                )\n",
        "        full_response = tokenizer.batch_decode(outputs)[0]\n",
        "        displayed_response = full_response if return_full_text else full_response.split(split_key)[-1].strip().replace(\"<|endoftext|>\", \"\").strip()\n",
        "        return displayed_response\n",
        "    except Exception as e:\n",
        "        return f\"The following error occured: {e}.\"\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question-answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Defining prompt templates\n",
        "DEFAULT_SYSTEM_PROMPT = \"Perfom the following instruction to the best of your ability.\"\n",
        "QA_PROMPT_TEMPLATE = 'Q:{instruction}\\nA:'\n",
        "DEFAULT_PROMPT_TEMPLATE = \"Q:{system_prompt}\\n{instruction}\\nA:\"\n",
        "\n",
        "PROMPT_TEMPLATE_SPACE = {\n",
        "    \"qa\": [QA_PROMPT_TEMPLATE, \"Instruct:{instruction}\\nOutput:\"],  # can add many more if needed\n",
        "    \"system_guided\": [DEFAULT_PROMPT_TEMPLATE, \"Instruct:{system_prompt}\\n{instruction}\\nOutput:\"] # same here can add many more\n",
        "}\n",
        "# Choose prompt as PROMPT_TEMPLATE_SPACE.get(\"qa\")[index] with index = 0  for example. So far the default prompt give better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Generate answer from instruction without system prompt\n",
        "def get_answer(instruction: str, include_prompt: bool = True) -> str:\n",
        "    prompt_template = PROMPT_TEMPLATE_SPACE.get(\"qa\")[0] # the first template gives better results\n",
        "    prompt = prompt_template.format(instruction=instruction)\n",
        "    return generate_response(prompt=prompt, return_full_text=include_prompt)\n",
        "\n",
        "\n",
        "# Generate answer from instruction with system prompt\n",
        "def get_system_guided_answer(instruction: str, include_prompt: bool = True) -> str:\n",
        "    prompt_template = PROMPT_TEMPLATE_SPACE.get(\"system_guided\")[0] # first template gives better results\n",
        "    prompt = prompt_template.format(system_prompt=DEFAULT_SYSTEM_PROMPT, instruction=instruction) \n",
        "    return generate_response(prompt=prompt, return_full_text=include_prompt)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "some_instructions = [\n",
        "    \"What is the capital city of Canada ?\", # Ottawa\n",
        "    \"In which city can we find the Eiffel tower ?\", #  Paris\n",
        "    \"Who was the US president between 2008 and 2012 ?\", # Obama\n",
        "    \"Quel est l'actuel président de la république française ?\", # Macron\n",
        "    \"What is the first prime number after 2024 ?\", # 2027\n",
        "    \"Give me a python function that finds out if an integer is a prime number.\",\n",
        "    \"Solve the following equation: 4x-10 = 6\", #x =4,\n",
        "    \"Solve this equation: 4.5x-10 = -1.5\" #x = 8.5/4.5 = 17/9 = 1.888888...\n",
        "\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/Users/hb/existing_repos/llms-notebooks/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q:What is the capital city of Canada?\n",
            "A: Ottawa.\n",
            "<|endoftext|>\n",
            "\n",
            "Q:Perfom the following instruction to the best of your ability.\n",
            "What is the capital city of Canada?\n",
            "A:The capital city of Canada is Ottawa.\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q:In which city can we find the Eiffel tower?\n",
            "A: Paris.\n",
            "<|endoftext|>\n",
            "\n",
            "Q:Perfom the following instruction to the best of your ability.\n",
            "In which city can we find the Eiffel tower?\n",
            "A: Paris, France is where the Eiffel Tower is located.\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q:Who was the US president between 2008 and 2012?\n",
            "A:Barack Obama.\n",
            "<|endoftext|>\n",
            "\n",
            "Q:Perfom the following instruction to the best of your ability.\n",
            "Who was the US president between 2008 and 2012?\n",
            "A:Barack Obama was the US President between 2008 and 2012.\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q:Quel est l'actuel président de la république française?\n",
            "A:L'actuel président de la république française est Emmanuel Macron.\n",
            "<|endoftext|>\n",
            "\n",
            "Q:Perfom the following instruction to the best of your ability.\n",
            "Quel est l'actuel président de la république française?\n",
            "A:The current president of France is Emmanuel Macron.\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q:What is the first prime number after 2024?\n",
            "A: The first prime number after 2024 is 2027.\n",
            "<|endoftext|>\n",
            "\n",
            "Q:Perfom the following instruction to the best of your ability.\n",
            "What is the first prime number after 2024?\n",
            "A:The first prime number after 2024 is 2027.\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q:Give me a python function that finds out if an integer is a prime number.\n",
            "A: def is_prime(n): \n",
            "    if n < 2: \n",
            "        return False\n",
            "    for i in range(2, int(n**0.5)+1): \n",
            "        if n % i == 0: \n",
            "            return False\n",
            "    return True\n",
            "<|endoftext|>\n",
            "\n",
            "Q:Perfom the following instruction to the best of your ability.\n",
            "Give me a python function that finds out if an integer is a prime number.\n",
            "A: def is_prime(n):\n",
            "    if n < 2:\n",
            "        return False\n",
            "    for i in range(2, int(n**0.5)+1):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q:Solve the following equation: 4x-10 = 6\n",
            "A: x = 4\n",
            "<|endoftext|>\n",
            "\n",
            "Q:Perfom the following instruction to the best of your ability.\n",
            "Solve the following equation: 4x-10 = 6\n",
            "A: 4x-10 = 6\n",
            "4x = 16\n",
            "Therefore, x = 4\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Q:Solve this equation: 4.5x-10 = -1.5\n",
            "A: x = 1.67\n",
            "<|endoftext|>\n",
            "\n",
            "Q:Perfom the following instruction to the best of your ability.\n",
            "Solve this equation: 4.5x-10 = -1.5\n",
            "A: 4.5x-10 = -1.5\n",
            "4.5x = 9.5\n",
            "Therefore, x = 2.1667\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for instruction in some_instructions:\n",
        "    print(get_answer(instruction=instruction))\n",
        "    print()\n",
        "    print(get_system_guided_answer(instruction=instruction))\n",
        "    print(3*\"\\n\")\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The prompt template `'Q:{instruction}\\nA:'` gives a very short answer compare to the system-guided prompt template `\"Q:{system_prompt}\\n{instruction}\\nA:\"` where `system_prompt=\"Perfom the following instruction to the best of your ability.\"`\n",
        "\n",
        "- Note that for solving equations it doesn't perform well when we use decimal numbers. This is not surprising at all. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##INPUT: Give me a summary of the following: Cher membre Amazon Prime,\n",
            "\n",
            "Nous vous écrivons pour vous informer des nouveautés concernant votre expérience Prime Video. À partir du 9 avril 2024, les films et séries Prime Video incluront de la publicité en quantité limitée*. Cela nous permettra de continuer à investir dans des contenus attractifs et d'augmenter nos investissements sur le long terme, afin de maintenir la qualité et la quantité des contenus sur Prime Video. Nous visons à avoir sensiblement moins de publicités que la télévision linéaire classique et les autres services de streaming. Il n'y a aucun changement concernant le prix actuel de votre abonnement Prime. Vous pouvez annuler votre abonnement gratuitement ou consulter votre prochaine date de renouvellement en visitant votre compte ici. Nous proposerons également une nouvelle option sans publicité pour €1,99 de plus par mois** à laquelle vous pouvez vous abonner ici.\n",
            "\n",
            "##OUTPUT: Amazon Prime is informing you about new features for your Prime Video experience starting April 9th, 2024. From now on, limited advertising will be included in films and series. This allows us to continue investing in attractive content and increase our investments over time, ensuring quality and quantity on Prime Video. We aim to have less advertising than linear television and other streaming services. There are no changes to the current price of your subscription. You can cancel your free account or renew at any time by visiting your account page here. We also offer a new option without advertising for $2.99 per month plus shipping costs.\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "def summarize(input_text: str) -> str:\n",
        "    prompt = f\"##INPUT: Give me a summary of the following: {input_text}\\n##OUTPUT:\"\n",
        "    return generate_response(prompt=prompt)\n",
        "\n",
        "input_text = \"\"\"Cher membre Amazon Prime,\n",
        "\n",
        "Nous vous écrivons pour vous informer des nouveautés concernant votre expérience Prime Video. À partir du 9 avril 2024, les films et séries Prime Video incluront de la publicité en quantité limitée*. Cela nous permettra de continuer à investir dans des contenus attractifs et d'augmenter nos investissements sur le long terme, afin de maintenir la qualité et la quantité des contenus sur Prime Video. Nous visons à avoir sensiblement moins de publicités que la télévision linéaire classique et les autres services de streaming. Il n'y a aucun changement concernant le prix actuel de votre abonnement Prime. Vous pouvez annuler votre abonnement gratuitement ou consulter votre prochaine date de renouvellement en visitant votre compte ici. Nous proposerons également une nouvelle option sans publicité pour €1,99 de plus par mois** à laquelle vous pouvez vous abonner ici.\n",
        "\"\"\"\n",
        "\n",
        "print(summarize(input_text=input_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09ecf3574401484d88a6ed721c5c81f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b28f5f10a24f43dca695797f39ee1672",
            "placeholder": "​",
            "style": "IPY_MODEL_e2d9dc99c50749a0a6b99c8d4cce3134",
            "value": " 2/2 [00:06&lt;00:00,  2.55s/it]"
          }
        },
        "53646783d6284efbad5b9f919c63cebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f2cb2c4f449463eb91b0dcd1c73d6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b034ccc3e097419fae21d8358ccf605a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53646783d6284efbad5b9f919c63cebf",
            "value": 2
          }
        },
        "609b6ef6e5824d398c47be87281f9ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b034ccc3e097419fae21d8358ccf605a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0c956be1c2c4cc3b5031e28fca4e00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f45fe69a282d4f0fa1433012c98ff340",
              "IPY_MODEL_5f2cb2c4f449463eb91b0dcd1c73d6e2",
              "IPY_MODEL_09ecf3574401484d88a6ed721c5c81f5"
            ],
            "layout": "IPY_MODEL_f6506b50b11040ba83b3e6a1b8b3bbe6"
          }
        },
        "b28f5f10a24f43dca695797f39ee1672": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2d9dc99c50749a0a6b99c8d4cce3134": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9233c69c0d5441783b85dbc3f633b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f45fe69a282d4f0fa1433012c98ff340": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9233c69c0d5441783b85dbc3f633b7c",
            "placeholder": "​",
            "style": "IPY_MODEL_609b6ef6e5824d398c47be87281f9ff7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f6506b50b11040ba83b3e6a1b8b3bbe6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
